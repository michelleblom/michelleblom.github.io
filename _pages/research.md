---
title: "Research"
layout: gridlay
sitemap: false
permalink: /research/
---

<style>
img{
  border-radius: 10px;
}
.col-md-3 {
  margin-top:10px;
  margin-bottom:10px;
  padding:0px;
  display:block;
  overflow:hidden;
  text-align:center;
  display: table-cell;
  background: white;
  border-radius: 20px;
  height: auto;
}
iframe {
  margin:0;
  padding:0;
  width: 175px;
  display: inline;
  vertical-align: middle;
}
</style>

## Research

I have diverse research interests that range from developing techniques for post-election audits, designing algorithms for solving complex optimization problems that arise in the resources sector, and exploring approaches for explaining decisions made by autonomous agents in varying contexts. I have expanded on these three themes -- Election Integrity, Optimization for Mine Planning, and Explainable AI -- below, describing some of the work that I, and my collaborators, have done/are doing in these areas. Much of my research, with the work on election auditing being the exception, involves collaboration with various industry partners, ranging from Rio Tinto (mine planning optimization), DST group (explainable AI and reinforcement learning for defence applications), and Mitsubishi Heavy Industries (factory scheduling). 

<div class="jumbotron">
<div class="col-md-12 col-sm-12">
<h4>Election Integrity</h4>

Elections are a cornerstone of democracy. They face new risks stemming from increasing
digitisation, cyber-security challenges and foreign interference. Guaranteeing that election results reflect
voters’ intentions is a foundational security issue. Election outcomes should be accompanied by evidence that they accurately reflect the will of the
voters. Post-election audits provide that evidence. 

With a team of researchers across Australia, the United States, and Europe, we have developed techniques for conducting risk-limiting post election audits (RLAs) for a range of election types. RLAs  guarantee a high probability of correcting incorrect reported election results, when generated by a combination of computer systems, independent of why the result was incorrect. RLAs for first-past-the-post elections are well understood, and have been used for some US elections for quite some time. Some US jurisdictions require a post-election RLA to certify the reported outcome. These audits work by randomly sampling a collection of the paper ballots that have been cast by voters. As these ballots are sampled, and examined, auditing software computes and maintains a <i>risk</i>, representing the probability that, if we stopped sampling ballots right now, we will have failed to detect that the reported outcome is in fact wrong.  Depending on the type of audit being conducted, we may be comparing a manual interpretation of a paper ballot to a digital cast vote record or CVR (where ballots have been scanned, and we can link the resulting CVRs to the original paper ballots). Differences in interpretation then feed into our calculation of risk. Alternatively, we may just be examining the paper ballots, in instances where CVRs are not available or cannot be linked back to paper ballots, and interpreting what we see to build evidence in support of (or against) the reported outcome. 

Our work has discovered how we can conduct RLAs for more complex elections, including Instant Runoff Voting (IRV), commonly referred to as Ranked Choice Voting (RCV).  There has been an ongoing effort to transition to IRV/RCV in a number of US jurisdictions, with preferential voting having a range of advantages over first-past-the-post. With the expectation that an increasing number of  IRV/RCV elections will take place in the US in the coming years, the ability to conduct RLAs for this kind of election is important. Our method, called RAIRE, is the first, and currently only, method for conducting practical RLAs of IRV/RCV elections.

Currently, we are actively working on post-election audits for Single Transferable Vote (STV) elections. STV is used to elect representatives to the upper houses of Australian state and federal parliaments. Our team has recently been awarded an ARC Discovery Project grant for the development of auditing methods for the Australian Senate. Ballots cast in Senate elections are scanned and digitised for counting using software implementing the STV algorithm. Two aspects of the process make effective audit design particularly challenging. First is the array of choices on how a voter can validly complete their ballot paper – not all boxes need to be checked, and it is not uncommon for a paper to involve up to 200 candidates. Second, vote counting can, and often does, entail a multitude of stages of progressive elimination of candidates and redistribution of their preferences. Further, small changes in the allocation of preferences on a small number of ballots can lead to dramatic shifts in the election outcome. We have developed RLAs for two seat STV elections where the first seat is awarded in the first round of counting. However, RLAs for STV in general is still an open research question.

In addition to IRV/RCV and STV, we have also developed RLAs for other election types, including Hamiltonian (used for Presidential primaries), Party-List Proportional, and Ranked Pairs 
elections.

<b>Collaborators:</b> Peter J. Stuckey, Vanessa J. Teague, Damjan Vukcevic, Alexander Ek, Philip B. Stark, Ron Rivest, and Jurlind Budurushi.
</div>
</div>

<div class="jumbotron">
<div class="col-md-12 col-sm-12">
<h4>Optimization for Open-Pit Mines</h4>

In my first post-Phd research position, I worked with Rio Tinto Iron Ore (RTIO) on an ARC Linkage Project (2012-15) exploring new algorithms for multi-objective short-term mine planning, for both single mine sites, and networks of sites connected by rail to a port system. Much of the work we did on this project focused on short-term planning horizons (up to 13 weeks). The methods we developed decomposed the planning problems into smaller ones that could be modelled as mixed integer programs (MIPs) and solved quickly. When scheduling a single mine, it was the planning horizon that was simplified. The weekly time periods were aggregated in various ways, including grouping later periods into larger chunks when scheduling earlier periods, with each week scheduled in turn while still considering the future albeit at a coarser granularity. When scheduling a multiple mine, rail and port system, we developed an iterative algorithm that solved mine- and port-side problems. Mines generated schedules to meet specific targets on product composition, while the port-side problem selected schedules to be 'enacted' at each mine and provided revised product targets to each mine in subsequent iterations. Long-term (life of mine) planning was considered toward the end of this project, with the key interest being the use of large neighbourhood search (LNS) techniques.

Across 2020-22, we revisited the idea of using LNS for solving long-term mine planning problems was revisited. There is precedent in the literature for using LNS on these kinds of problems, although the neighbourhood structures applied in existing work were generally not very effective on heavily constrained problems. The models we considered involved multiple mines, rail, and a port system, together with capital decisions and tight constraints. We looked at the kinds of constraints that we had in our models, such as blending and minimum/maximum production constraints, and designed neighbourhood structures to maximise the opportunities for finding new and improved solutions. This work was particularly successful, with the resulting algorithm able to solve problems in hours that previously took days of solving.  

In terms of where I would like to take this work, I have a particular interest in multi-horizon planning. In the resources, and other, sectors it is common to plan at different horizons, ranging from longer-term more strategic planning to shorter-term horizons. Planning at one horizon will typically guide the plans at the shorter-term.  If we consider the mining sector, long-term plans aim to maximise the net present value (NPV) of a project, yet it's at the short-term where this value will either be realised or lost. Current practice does not connect planning models across horizons in any meaningful way, and when used discretely these models are less than optimal. With connected models, planners would be able to analyse the impact of decisions at one horizon on those above and below, and be guided towards short-term decisions that are, for example, the least destructive to longer-term value.  

<b>Collaborators:</b> Adrian R. Pearce, Peter J. Stuckey, Rio Tinto.
</div>
</div>

<div class="jumbotron">
<div class="col-md-12 col-sm-12">
<h4>Explainable AI</h4>

My work in this space has, thus far, focused on either explaining the reasoning of a system that arrives at conclusions (beliefs about an environment) on the basis of various sources of information, or explaining why a reinforcement-learning based agent made the recommendations it did in the context of a game.  In the former context, the system we were working with was called Consensus. This system, given a knowledge-base of observations, background knowledge, and logical rules, made inferences about the state of an environment and generated accompanying reasoning traces. An example of such an inference may be that a certain vessel was engaging in illegal fishing activities at a particular location. We then looked at how, given a usually quite complex logical reasoning trace, we could explain how the system arrived at that conclusion. The methods we considered, and ultimately developed, were grounded in existing work on simplifying and rewriting logical proofs, or expressing these proofs in natural language, and provenance-based reasoning/visualisation systems. User studies were conducted to understand the kinds of information that people found useful or meaningful in an explanation of a Consensus-generated conclusion. In the second context, our aim was to examine how explanations could help human players learn specific strategies for playing a game. The game we considered in this case was called Joadia, and was based on a disaster-relief scenario. In this case, our work focused on the use of contrastive explanations, at varying levels of detail. Through user studies, we looked at how different kinds of explanation influenced human players in their ability to make the best decisions at various points in the game.

<b>Collaborators:</b> Tim Miller, Ronal Singh, Emma Baillie, DST Group.
</div>
</div>
